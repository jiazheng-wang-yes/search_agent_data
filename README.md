# search_agent_data

For each attack method, there are output files and LLM-as-a-Judge results. Each model should have baseline run, with secure prompt (defense method 1) and with reflection module (defense method 2).

The poisoned QA pairs are in `./poison_fake_qa_pairs.json`
The original QA pairs are in `./qa_pairs_subset_50.json`
